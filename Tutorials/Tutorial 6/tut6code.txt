import numpy as np 
import matplotlib.pyplot as plt

x = np.linspace(0, 1, 200)

#set up a function of x 
y = x.copy() 
#^ note that we don't actually end up using y = x 
# anywhere, but it does set up the array
y[x < 0.1] = 0 #y = 0 for x < 0.1
y[x >= 0.1] = 0.5*(x[x >= 0.1]-0.1) #add a gradient of +0.5 if x >= 0.5
y[x >= 0.6] = 0.25-0.25/0.2*(x[x >= 0.6]-0.6) 
#^ gradient downwards from 0.6 to 0.8 (gradient 
#calculated so that line goes from 
#(0.6, 0.25) -> (0.8, 0), i.e. joins up with the other segments)
y[x > 0.8] = 0 #y = 0 for x > 0.8

fig, ax = plt.subplots() 
plt.plot(x,y)
---
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense

#set up a sequential neural network
model = Sequential()

#add a layer of 3 nodes of ReLUs, taking a single parametric input
model.add(Dense(units=3, activation='relu', input_dim=1))
#add a linear node at the end to combine the nodes together
model.add(Dense(units=1, activation='linear'))

#compile the model, trying to minimise mean squared 
#error and using the Adam algorithm to fit this
model.compile(loss="mean_squared_error",
              optimizer='adam')

#fit the data provided previously, using 200 epochs and a batch size of 32
model.fit(x, y, epochs=200, batch_size=32)

#obtain a predicted set of values from the fitted function along its length
y_pred = model.predict(x)
---
model.layers[0].set_weights([np.array([[0.5, -(-0.5-0.25/0.2), 0.25/0.2]],), np.array([-0.5*0.1, (-0.5-0.25/0.2)*0.6, -(0.25/0.2)*0.8],)]) 
model.layers[1].set_weights([np.array([[1],[-1],[1]],), np.array([0],)])
---
model = Sequential()

model.add(Dense(units=4, activation='relu', input_dim=2)) 
model.add(Dense(units=4, activation='relu')) 
model.add(Dense(units=2, activation='softmax')) 

model.compile(loss='categorical_crossentropy',
              optimizer='sgd')
---
from tensorflow.keras.utils import *
...
y_binary = to_categorical(y)
---
model.fit(X, y_binary, epochs=250, batch_size=32)
---
model.save('model.h5')
---
from google.colab import files
files.download(...)