{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggp-0oyVUo4M"
   },
   "source": [
    "# ME4 Machine Learning - Tutorial 5\n",
    "Lecture 5 covered some of the theory behind Support Vector Machines (SVMs). In this tutorial you will use Python and Scikit-learn to perform classification via SVMs to get practical experience of the theory covered in the lecture. You will generate some test datasets, and you will also be able to explore the different kernel types discussed in the lecture to see what effect these have on the classification performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUYrdEtXU5Z5"
   },
   "source": [
    "First let's get some functions defined - some of these you may have seen before - these are just to help us with the rest of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktf471zM8zzZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#define the grid to sample the domain as in previous tutorials\n",
    "def gen_sample_grid(npx=200, npy=200, limit=1):\n",
    "  x1line = np.linspace(-limit, limit, npx)\n",
    "  x2line = np.linspace(-limit, limit, npy)\n",
    "  x1grid, x2grid = np.meshgrid(x1line, x2line)\n",
    "  Xgrid = np.array([x1grid, x2grid]).reshape([2,npx*npy]).T\n",
    "  return Xgrid,x1line,x2line\n",
    "\n",
    "#get a covariance matrix at an angle\n",
    "def get_cov(sdx=1., sdy=1., rotangdeg=0.):\n",
    "    covar = np.array([[sdx, 0], [0, sdy]])\n",
    "    rot_ang = rotangdeg / 360 * 2 * np.pi\n",
    "    rot_mat = np.array([[np.cos(rot_ang), -np.sin(rot_ang)], [np.sin(rot_ang), np.cos(rot_ang)]])\n",
    "\n",
    "    covar = np.matmul(np.matmul(rot_mat, covar), rot_mat.transpose())\n",
    "    return covar\n",
    "\n",
    "#generate an xor distribution\n",
    "def gen_xor_distribution(n=100):\n",
    "    a = np.round(n / 4).astype('int')\n",
    "    b = n-a*3\n",
    "    xc1 = np.concatenate([np.random.multivariate_normal([-2.3, -2.3], get_cov(0.4, 0.1, -45), a),\n",
    "                          np.random.multivariate_normal([2.3, 2.3], get_cov(0.4, 0.1, -45), a)])\n",
    "    xc2 = np.concatenate([np.random.multivariate_normal([-2.3, 2.3], get_cov(0.4, 0.1, 45), a),\n",
    "                          np.random.multivariate_normal([2.3, -2.3], get_cov(0.4, 0.1, 45), b)])\n",
    "    xc = np.array(np.concatenate([xc1, xc2]))\n",
    "\n",
    "    y = np.array(np.concatenate([np.zeros([2 * a, 1]), np.ones([a+b, 1])])).squeeze()\n",
    "    X = xc\n",
    "    return X, y\n",
    "\n",
    "#generate a circular distribution of points\n",
    "def gen_circular_distribution(n=500, scale=1):\n",
    "    a = np.round(n / 7).astype('int')\n",
    "    b = np.round(2*n / 7).astype('int')\n",
    "    c = n - a - b\n",
    "    r1 = np.concatenate(\n",
    "        [np.random.normal(loc=2, scale=scale, size=[a, 1]), np.random.normal(loc=8, scale=scale, size=[c, 1])])\n",
    "    r2 = np.random.normal(loc=5, scale=scale, size=[b, 1])\n",
    "\n",
    "    th1 = np.random.uniform(low=0, high=2 * np.pi, size=[a+c, 1])\n",
    "    th2 = np.random.uniform(low=0, high=2 * np.pi, size=[b, 1])\n",
    "\n",
    "    x1a = r1 * np.cos(th1)\n",
    "    x2a = r1 * np.sin(th1)\n",
    "\n",
    "    x1b = r2 * np.cos(th2)\n",
    "    x2b = r2 * np.sin(th2)\n",
    "\n",
    "    X = np.concatenate([np.concatenate([x1a.reshape([a+c, 1]), x1b.reshape([b, 1])]),\n",
    "                        np.concatenate([x2a.reshape([a+c, 1]), x2b.reshape([b, 1])])], axis=1)\n",
    "\n",
    "    y = np.concatenate([np.zeros([a+c, 1]), np.ones([b, 1])]).squeeze()\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwUlB20UZ6SF"
   },
   "source": [
    "#XOR dataset\n",
    "We will try to perform classification on an XOR dataset (as shown in the lecture). This is a classic example which cannot be classified by a simple linear discriminator. Generate a dataset using the ‘gen_xor_distribution()’ function with a parameter of 400. Do a scatter plot to check that it looks sensible. It would be a good idea to make sure that this is producing the same dataset each time it is run, so you may wish to add ‘np.random.seed(0)’ before calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t85TdO9VawSf"
   },
   "outputs": [],
   "source": [
    "#Set the random seed for consistency:\n",
    "np.random.seed(0)\n",
    "\n",
    "### your code here... ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RUuXjPPa3BC"
   },
   "source": [
    "Next you can import the necessary SVM Scikit learn libraries and get them set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zU79kqDka2OK"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Get your SVM set up using the SVC library. \n",
    "svm = SVC(C=0.01, gamma='auto', kernel='poly', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRI3UwdhbalX"
   },
   "source": [
    "Note that here we're using polynomial basis functions, degree 2 (matching what we did in the lecture). Use svm.fit() to fit the training XOR dataset you've just generated (note - look up the syntax in the online documentation but note that this is very similar for the scikit-learn tools) then use it to predict classification values at every point in a grid of 200 $\\times$ 200 pixels from -4 to 4 in each direction - plot these. Also plot the scattered training data on top of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP3XgmnmcJm1"
   },
   "outputs": [],
   "source": [
    "#name the figure as my_fig (we use this again later when we add things to the plot) - do not define a separate plot!\n",
    "my_fig, ax = plt.subplots()\n",
    "\n",
    "### your code here:\n",
    "#svm.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj79b3qhcWqD"
   },
   "source": [
    "The SVC has several methods for extracting useful analysis information. SVC.decision_function() can be used to calculate the decision function throughout; its form matches the classification output from SVC.predict(), so can be plotted in exactly the same way. Plot contours at -1, 0 and 1, corresponding to the margin positions and the decision boundary itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsM43yOHca5Y"
   },
   "outputs": [],
   "source": [
    "Z = np.reshape(svm.decision_function(Xgrid), [npx, npy])\n",
    "ax.contour(x1line, x2line, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "#this line reloads the figure\n",
    "my_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qey3nSKKclAb"
   },
   "source": [
    "Here, x1line and x2line are the two coordinates for which the grid is defined and Z is the decision function, reshaped to match the coordinates, Xgrid is the combination of the gridding vectors, defined via numpy.meshgrid() as in previous tutorials, and npx and npy are both 200 for the number of pixels in x and y respectively. In this plot, the two dashed lines define the extent of the margin. The solid line is the classification boundary itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViHh0iDoc5Ao"
   },
   "source": [
    "We now want to plot which points are the support vectors. The property sv = svm.support_vectors_ gives the support vectors. Plot these points on the scatter plot too with black crosses (marker=\"x\", c=\"#000000\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgjKgaeaxY5y"
   },
   "outputs": [],
   "source": [
    "### your code here:\n",
    "\n",
    "\n",
    "\n",
    "#reload the figure\n",
    "my_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97Gu6gRkxX3s"
   },
   "source": [
    "You should notice that several of the support vectors lie within the margin region (although in this case, none cross the boundary itself). Why do some of these points cross the margin edge? Based on what we looked at in the lecture, what could you do to stop this happening? Note that we will find out about the specific scikit-learn parameters in a minute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8JJr730dJFu"
   },
   "source": [
    "---\n",
    "Now try out different functions. First of all see what happens with linear mapping (kernel = 'linear') for this system. (Hint - you may find it easiest to edit the line above and rerun all cells with Ctrl+F9 rather than copying all the code.)  Does this behaviour match what you'd expect? Why? The degree=2 parameter for the polynomial kernel should be self-explanatory – try switching to a fourth-order polynomial instead. What do you think about the result? \n",
    "\n",
    "Then try varying C (how hard the boundary is) although note that there is a different definition here from what we discussed in the lecture - small C allows large overlaps, while large C fits the data very tightly (hard boundary) – this is the reciprocal behaviour of what we were considering. For C=1000 with kernel='poly', degree=2, you should see that there are just the support vectors needed to define the margin boundaries. This works well with well separated data, but could be more tricky if the data overlaps. \n",
    "\n",
    "Also try kernel='rbf' - this is a very common choice - the radial basis functions. Again, adjust the C value and see the effect on the resulting decision surface and the support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKFcLHzxdU5G"
   },
   "source": [
    "---\n",
    "# Circular dataset\n",
    "\n",
    "In tutorial 1 you may have done the task to define a circular dataset. This is another example which is a challenge to segment via linear classifiers. The functions provided include ‘gen_circular_distribution’ which will generate the distribution for you. Use 200 points, and fit a SVM to the data. Again, try some different kernel functions and parameters to see how they perform. Note that you should plot across the range -10 to 10 in each dimension if plotting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJExxKlpdm-p"
   },
   "outputs": [],
   "source": [
    "### your code here... ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq42Xn2--q-s"
   },
   "source": [
    "#Performance evaluation\n",
    "\n",
    "For this part, increase the number of points in the dataset to 500. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSIHSsqZ9FC2"
   },
   "outputs": [],
   "source": [
    "X,y = gen_circular_distribution(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMW5DrIi9EC1"
   },
   "source": [
    "We will now demonstrate how overfitting will behave as well as learning how to implement the k fold cross-validation approach (discussed in lecture 3).\n",
    "\n",
    "We will use the k fold approach to test the performance for several different parameters. Scikit-learn already has tools to achieve this built in. You can set this up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqS0_nkSc0ET"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4V9Bgesdzb9"
   },
   "source": [
    "Here, n_splits defines the number of splits, and shuffle means that the data points are shuffled before being split, i.e. the points are randomly allocated to each split. \n",
    "\n",
    "In the following section, write code for the commented sections where indicated. Use RBF kernels for your SVMs. When you loop through to compare the data, you may wish to reuse code from one of the earlier tutorials; we did something similar in tutorial 2. You should average your five sets of performance values together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGKsJRy-d6HP"
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "\tX_train = X[train_index]\n",
    "\ty_train = y[train_index]\n",
    "\tX_test = X[test_index]\n",
    "\ty_test = y[test_index]\n",
    "\n",
    "\t#use X_train, y_train to train the SVM\n",
    "\t### your code here... ###\n",
    "\t\n",
    "  #use svm.predict() to predict the output for the test data set\n",
    "\t### your code here... ###\n",
    "\t\n",
    "  #loop through to compare the test data output to what it should be \n",
    "\t#\tand obtain the fraction of correct classifications)\n",
    "\t### your code here... ###\n",
    "\t\n",
    "  #do the same prediction and performance assessment performance \n",
    "\t#\twith the training data\n",
    "\t### your code here... ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjnFYO9WeDoD"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AJhFPDAeOP5"
   },
   "source": [
    "---\n",
    "More detail about the code (if you are interested!)\n",
    "\n",
    "The for loop extracts pairs of indices from the kf.split() function. We get 5 sets of these - corresponding to the number of splits identified - train_index and test_index are set to these. If you want to check the sizes of these, you should see that train_index has 80% of the indices, and test_index the remaining 20%. These indices will be different parts for each loop. We use the indices to define the training and testing datasets in the first four lines within the loop. \n",
    "\n",
    "Training on the training dataset should be straightforward, and predicting with the predict() function similarly. This should output a variable comparable to y_test, and you should compare each element by looping through.\n",
    "\n",
    "The last part, assessing the performance with the training data is less necessary, however, it is useful to help judge overfitting and whether there is any discrepance between the training fitting and the performance with the test dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "976U1llOeaFA"
   },
   "source": [
    "Observe the results for training and testing data when C = 0.03, C = 0.2, C = 2, C = 20. What would you expect to happen to these errors as C varies? What does this mean for bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcrX8GHneb6b"
   },
   "source": [
    "If you have time and want to investigate more, you can try to plot a graph as a function of C. In order to investigate the behaviour of the performance for both the training and testing data fully, you can add another loop around the outside of this to vary C automatically; it is wise to use a power law to sample, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6leSeRcNehft"
   },
   "outputs": [],
   "source": [
    "C_array = np.power(10, np.linspace(-1.5, 1.5, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G90MMTr-ejr7"
   },
   "source": [
    "This will generate eight of values between $10^{-1.5}$ and $10^{1.5}$. Produce a plot of the two values as a function of C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mmGqWzjAbHi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yg_BCtQlAbdt"
   },
   "source": [
    "Does this match your expectation? Based on this, what would you set your value of C to be? Identify where bias and variance would be high and low respectively."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
